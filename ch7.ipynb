{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "# 硬投票分类器\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_pred,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DecisionTreeClassifier 0.816\n",
      "BaggingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "# bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),n_estimators=500,bootstrap=True,n_jobs=-1)\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "for clf in (tree_clf, bag_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_pred,y_val))"
   ]
  },
  {
   "source": [
    "# AdaBoost\n",
    "\n",
    "一个跟节点和两个叶子节点组成的树称为树桩stump\n",
    "\n",
    "AdaBoost具有三个特点：\n",
    "\n",
    "1. AdaBoost由许多弱学习器组成，弱学习器一般为stump\n",
    "2. 其中一些stumps具有更多的话语权\n",
    "3. 每个stump的生成都考虑了前一个stump的错误\n",
    "\n",
    "## 基本算法\n",
    "\n",
    "1. 为每一个样本分配一个样本权重，1/样本数量。\n",
    "2. 创建第一个stump，选择一个属性作为决策变量（基尼系数）。\n",
    "3. 生成stump的权重\n",
    "    1. total error 是所有分类错误的样本的样本权重和\n",
    "    2. 学习器权重$=1/2\\log{(1-total error)/total error}$\n",
    "4. 修改样本的样本权重\n",
    "    1. 增加错误样本的权重 $sample weight = sample weight* e^{学习器权重}$\n",
    "    2. 降低的样本权重 $sample weight = sample weight* e^{-学习器权重}$\n",
    "    3. 对所有的权重归一化，是他们的和为1 \n",
    "5. 按照样本权重生成新的样本集，大小和原来的集合一样，然后回到第一步创建下一个stump（或者使用weighted gini function 生成下一个stump）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DecisionTreeClassifier 0.84\n",
      "AdaBoostClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1) , n_estimators=300,\n",
    "    learning_rate=0.2)\n",
    "for clf in (tree_clf, ada_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_pred,y_val))"
   ]
  },
  {
   "source": [
    "# 梯度提升\n",
    "\n",
    "与AdaBoost不同的是，它不是在每个迭代中调整实例权重，而是让新预测器对前一个预测器的残差进行拟合。\n",
    "\n",
    "最后对所有的预测器的预测结果相加就是对新实例的预测。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DecisionTreeClassifier 0.848\n",
      "[20:34:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier 0.872\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBClassifier()\n",
    "for clf in (tree_clf, xgb_reg):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_pred,y_val))"
   ]
  },
  {
   "source": [
    "# 堆叠法\n",
    "\n",
    "其基本想法在于不是通过一个简单的函数（比如硬投票）集成所有预测器的预测值，而是训练一个模型去执行这个聚合。\n",
    "\n",
    "通过留存法，将训练集分成多个子集，第一个子集训练第一层，\n",
    "\n",
    "第一层的预测器在第二个子集上进行预测，然后将预测值作为特征训练第二层预测器，以此类推可以训练多层堆叠集成的预测器。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}